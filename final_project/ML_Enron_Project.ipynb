{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Dataset Exploration and Experimentation Log\n",
    "\n",
    "First, I got the dimensions of the data, the column names, the count of missing values in each column, and the number of POIs and non-POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 21)\n",
      "salary                        51\n",
      "to_messages                   60\n",
      "deferral_payments            107\n",
      "total_payments                21\n",
      "exercised_stock_options       44\n",
      "bonus                         64\n",
      "restricted_stock              36\n",
      "shared_receipt_with_poi       60\n",
      "restricted_stock_deferred    128\n",
      "total_stock_value             20\n",
      "expenses                      51\n",
      "loan_advances                142\n",
      "from_messages                 60\n",
      "other                         53\n",
      "from_this_person_to_poi       60\n",
      "poi                            0\n",
      "director_fees                129\n",
      "deferred_income               97\n",
      "long_term_incentive           80\n",
      "email_address                 35\n",
      "from_poi_to_this_person       60\n",
      "dtype: int64\n",
      "False    128\n",
      "True      18\n",
      "Name: poi, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi', 'salary', 'prop_messages_with_poi', 'total_payments', 'exercised_stock_options',\n",
    "                'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'total_stock_value', 'expenses',\n",
    "                'loan_advances', 'other', 'long_term_incentive'] \n",
    "# Since the dataset is so small, I'll start with all features with at least fifty non-NaN values, \n",
    "# except for email_address\n",
    "# Replace message count fields with prop_messages_with_poi (created below)\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# explore dataset\n",
    "data_df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "print data_df.shape\n",
    "# print data_df.columns\n",
    "data_df = data_df.replace('NaN', np.nan)\n",
    "print data_df.isnull().sum()\n",
    "print data_df['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know from the mini-projects that 'TOTAL' should be removed since it's not a real data point. If I find more outliers later on, I'll remove them here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "### Task 2: Remove outliers\n",
    "data_dict.pop('TOTAL', '')\n",
    "print len(data_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created three new columns: \n",
    "* total_messages, the sum of from_messages and to_messages\n",
    "* total_poi_messages, the sum of from_this_person_to_poi and from_poi_to_this_person\n",
    "* prop_messages_with_poi, the proportion of total_poi_messages to total_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to_messages', 'deferral_payments', 'expenses', 'poi', 'deferred_income', 'email_address', 'long_term_incentive', 'total_poi_messages', 'prop_messages_with_poi', 'restricted_stock_deferred', 'shared_receipt_with_poi', 'loan_advances', 'from_messages', 'other', 'director_fees', 'total_messages', 'bonus', 'total_stock_value', 'from_poi_to_this_person', 'from_this_person_to_poi', 'restricted_stock', 'salary', 'total_payments', 'exercised_stock_options']\n"
     ]
    }
   ],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "for person in data_dict:\n",
    "    if data_dict[person]['to_messages'] != 'Nan' and data_dict[person]['from_messages'] != 'Nan':\n",
    "        data_dict[person]['total_messages'] = data_dict[person]['to_messages'] + data_dict[person]['from_messages']\n",
    "    else:\n",
    "        data_dict[person]['total_messages'] = 'NaN'\n",
    "    \n",
    "    if data_dict[person]['from_poi_to_this_person'] != 'NaN' and data_dict[person]['from_this_person_to_poi'] != 'NaN':\n",
    "        data_dict[person]['total_poi_messages'] = data_dict[person]['from_this_person_to_poi'] + data_dict[person]['from_poi_to_this_person']\n",
    "    else:\n",
    "        data_dict[person]['total_poi_messages'] = 'NaN'\n",
    "    \n",
    "    if data_dict[person]['total_messages'] != 'Nan' and data_dict[person]['total_poi_messages'] != 'NaN':\n",
    "        data_dict[person]['prop_messages_with_poi'] = float(data_dict[person]['total_poi_messages']) / float(data_dict[person]['total_messages'])\n",
    "    else:\n",
    "        data_dict[person]['prop_messages_with_poi'] = 'NaN'\n",
    "    \n",
    "print data_dict['SKILLING JEFFREY K'].keys()\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll do feature scaling since I'm going to be trying a few different algorithms, some of which rely on the distance between points in the feature space.\n",
    "\n",
    "Then, I'll do a simple train/test split and try out some classifiers with all of these features. I'll look at the accuracy score, precision, and recall to get an idea of which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/utils/deprecation.py:52: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.43625744  0.18139418  0.13326897  0.09829136  0.05426126  0.03976898\n",
      "  0.01998366  0.0182827 ]\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a variety of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(scaled_features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "### PCA\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "pca = RandomizedPCA(n_components=8, whiten=True).fit(features_train)\n",
    "pca.fit(features_train)\n",
    "pca_features_train = pca.transform(features_train)\n",
    "pca_features_test = pca.transform(features_test)\n",
    "print pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.909090909091\n",
      "precision:  0.6\n",
      "recall:  0.6\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.95      0.95        39\n",
      "        1.0       0.60      0.60      0.60         5\n",
      "\n",
      "avg / total       0.91      0.91      0.91        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "### Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(pca_features_train, labels_train)\n",
    "pred = nb_clf.predict(pca_features_test)\n",
    "print \"accuracy: \", nb_clf.score(pca_features_test, labels_test)\n",
    "\n",
    "print \"precision: \", precision_score(labels_test, pred)\n",
    "print \"recall: \", recall_score(labels_test, pred)\n",
    "\n",
    "print classification_report(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.886363636364\n",
      "precision:  0.0\n",
      "recall:  0.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      1.00      0.94        39\n",
      "        1.0       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.79      0.89      0.83        44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC(kernel='rbf')\n",
    "svm_clf.fit(pca_features_train, labels_train)\n",
    "pred = svm_clf.predict(pca_features_test)\n",
    "print \"accuracy: \", svm_clf.score(pca_features_test, labels_test)\n",
    "\n",
    "print \"precision: \", precision_score(labels_test, pred)\n",
    "print \"recall: \", recall_score(labels_test, pred)\n",
    "\n",
    "print classification_report(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.931818181818\n",
      "precision:  0.75\n",
      "recall:  0.6\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.97      0.96        39\n",
      "        1.0       0.75      0.60      0.67         5\n",
      "\n",
      "avg / total       0.93      0.93      0.93        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(pca_features_train, labels_train)\n",
    "pred = dt_clf.predict(pca_features_test)\n",
    "print \"accuracy: \", dt_clf.score(pca_features_test, labels_test)\n",
    "\n",
    "print \"precision: \", precision_score(labels_test, pred)\n",
    "print \"recall: \", recall_score(labels_test, pred)\n",
    "\n",
    "print classification_report(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.909090909091\n",
      "precision:  0.666666666667\n",
      "recall:  0.4\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.93      0.97      0.95        39\n",
      "        1.0       0.67      0.40      0.50         5\n",
      "\n",
      "avg / total       0.90      0.91      0.90        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state=100)\n",
    "rf_clf.fit(pca_features_train, labels_train)\n",
    "pred = rf_clf.predict(pca_features_test)\n",
    "print \"accuracy: \", rf_clf.score(pca_features_test, labels_test)\n",
    "\n",
    "print \"precision: \", precision_score(labels_test, pred)\n",
    "print \"recall: \", recall_score(labels_test, pred)\n",
    "\n",
    "print classification_report(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.909090909091\n",
      "precision:  0.6\n",
      "recall:  0.6\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.95      0.95        39\n",
      "        1.0       0.60      0.60      0.60         5\n",
      "\n",
      "avg / total       0.91      0.91      0.91        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ab_clf = AdaBoostClassifier()\n",
    "ab_clf.fit(pca_features_train, labels_train)\n",
    "pred = ab_clf.predict(pca_features_test)\n",
    "print \"accuracy: \", ab_clf.score(pca_features_test, labels_test)\n",
    "\n",
    "print \"precision: \", precision_score(labels_test, pred)\n",
    "print \"recall: \", recall_score(labels_test, pred)\n",
    "\n",
    "print classification_report(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.840909090909\n",
      "precision:  0.0\n",
      "recall:  0.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.95      0.91        39\n",
      "        1.0       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.78      0.84      0.81        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(pca_features_train, labels_train)\n",
    "pred = knn_clf.predict(pca_features_test)\n",
    "print \"accuracy: \", knn_clf.score(pca_features_test, labels_test)\n",
    "\n",
    "print \"precision: \", precision_score(labels_test, pred)\n",
    "print \"recall: \", recall_score(labels_test, pred)\n",
    "\n",
    "print classification_report(labels_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these basic supervised classifiers, the best accuracy performers are SVM, Random Forest, and K Nearest Neighbors. All of these give an accuracy of around 88.6%. Random Forest and Naive Bayes are giving the best precision/recall scores.\n",
    "\n",
    "After going back and adding feature scaling and PCA (8 components gives the best result), the precision and recall scores have improved. The classifiers currently performing best are Naive Bayes, Random Forest, and AdaBoost.\n",
    "\n",
    "Naive Bayes doesn't have parameters to tune, so I'll tune the other two and choose the best from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 2, 'n_estimators': 12, 'criterion': 'gini'}\n",
      "F1:  0.3\n"
     ]
    }
   ],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "# Tune Random Forest rf_clf\n",
    "# With default parameters:\n",
    "# accuracy:  0.909090909091\n",
    "# precision:  0.666666666667\n",
    "# recall:  0.4\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#         0.0       0.93      0.97      0.95        39\n",
    "#         1.0       0.67      0.40      0.50         5\n",
    "\n",
    "# avg / total       0.90      0.91      0.90        44\n",
    "\n",
    "parameters = {'n_estimators': (6, 8, 10, 12, 14, 20, 50),\n",
    "              'criterion': ('gini', 'entropy'),\n",
    "              'min_samples_split': (2, 4, 6)}\n",
    "sss = StratifiedShuffleSplit()\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(random_state=100), parameters, scoring=\"f1\", cv=sss)\n",
    "clf.fit(pca_features_train, labels_train)\n",
    "\n",
    "print clf.best_params_\n",
    "print \"F1: \", clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 10}\n",
      "F1:  0.233333333333\n"
     ]
    }
   ],
   "source": [
    "# Tune AdaBoost ab_clf\n",
    "parameters = {'n_estimators': (10, 20, 50, 70, 100)}\n",
    "sss = StratifiedShuffleSplit()\n",
    "\n",
    "clf = GridSearchCV(AdaBoostClassifier(), parameters, scoring=\"f1\", cv=sss)\n",
    "clf.fit(pca_features_train, labels_train)\n",
    "\n",
    "print clf.best_params_\n",
    "print \"F1: \", clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
